{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:22:43.292803Z",
     "start_time": "2019-03-03T13:22:42.827276Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch import nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc, classification_report, balanced_accuracy_score\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and transform data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:24:35.951208Z",
     "start_time": "2019-03-03T11:24:35.937606Z"
    }
   },
   "outputs": [],
   "source": [
    "seed=622\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:23:07.100410Z",
     "start_time": "2019-03-03T13:23:01.009135Z"
    }
   },
   "outputs": [],
   "source": [
    "colab_env = False\n",
    "\n",
    "if not(colab_env):\n",
    "#     train = pd.read_hdf('preprocessed.h5', 'train')\n",
    "    validation = pd.read_hdf('preprocessed.h5', 'validation')\n",
    "    X_resampled, y_resampled = pickle.load(open( \"resampled\", \"rb\" ))\n",
    "    \n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    validation = pd.read_hdf('/content/gdrive/My Drive/Colab Notebooks/we_data/preprocessed.h5', 'validation')\n",
    "    test = pd.read_hdf('/content/gdrive/My Drive/Colab Notebooks/we_data/preprocessed.h5', 'test')\n",
    "#     X_resampled, y_resampled = pickle.load(open( \"/content/gdrive/My Drive/Colab Notebooks/we_data/resampled\", \"rb\" ))\n",
    "    X_resampled, y_resampled = pickle.load(open( \"/content/gdrive/My Drive/Colab Notebooks/we_data/subsampled\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:32:01.787606Z",
     "start_time": "2019-03-03T13:31:52.146349Z"
    }
   },
   "outputs": [],
   "source": [
    "X_validation = validation.drop(columns=['click', 'payprice']).values\n",
    "y_validation = validation['click'].values\n",
    "    \n",
    "# X_train = train.drop(columns=['click', 'payprice']).values\n",
    "# y_train = train['click'].values\n",
    "                                              \n",
    "# weights = torch.DoubleTensor(1-(np.bincount(y_resampled))/len(y_resampled))                                   \n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 2) \n",
    "# samples = torch.utils.data.BatchSampler(sampler, batch_size=1000, drop_last=False)\n",
    "\n",
    "# print(X_resampled.shape[1] == X_validation.shape[1])\n",
    "\n",
    "# train_loader = TensorDataset(torch.from_numpy(X_resampled).float(), torch.from_numpy(y_resampled).float())\n",
    "train_loader = TensorDataset(torch.from_numpy(X_subsampled.values).float(), torch.from_numpy(y_subsampled).float())\n",
    "# train_loader = TensorDataset(torch.from_numpy(X_train.values).float(), torch.from_numpy(y_train).float())\n",
    "\n",
    "train_loader = DataLoader(train_loader, num_workers=4, batch_size=50000, shuffle=True)\n",
    "\n",
    "validation_loader = TensorDataset(torch.from_numpy(X_validation).float(), torch.from_numpy(y_validation).float())\n",
    "validation_loader = DataLoader(validation_loader, batch_size=40000, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:28:56.209744Z",
     "start_time": "2019-03-03T13:28:56.073294Z"
    }
   },
   "outputs": [],
   "source": [
    "# set seed fucntiosn\n",
    "seed=622\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set number of nodes per hidden layer\n",
    "input_layer = X_validation.shape[1]\n",
    "hidden_layer_1 = 2**11\n",
    "hidden_layer_2 = 2**11\n",
    "hidden_layer_3 = 2**11\n",
    "hidden_layer_4 = 2**11\n",
    "output_layer = 1\n",
    "\n",
    "# add linear layers and init weight function\n",
    "linear1 = nn.Linear(input_layer, hidden_layer_1)\n",
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "\n",
    "linear2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "\n",
    "linear3 = nn.Linear(hidden_layer_2, hidden_layer_3)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "linear4 = nn.Linear(hidden_layer_3, hidden_layer_4)\n",
    "nn.init.xavier_uniform_(linear4.weight)\n",
    "\n",
    "output = nn.Linear(hidden_layer_4, output_layer)\n",
    "nn.init.xavier_uniform_(output.weight)\n",
    "\n",
    "# dropout nodes when training the model to prevent over fitting\n",
    "drop_prob = 0.5\n",
    "dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "# set sequential NN model\n",
    "model = torch.nn.Sequential(\n",
    "    linear1,\n",
    "    dropout,\n",
    "    nn.PReLU(),\n",
    "    linear2,\n",
    "    dropout,\n",
    "    nn.PReLU(),\n",
    "    linear3,\n",
    "    dropout,\n",
    "    nn.PReLU(),\n",
    "    linear4,\n",
    "    dropout,\n",
    "    nn.ReLU(),\n",
    "    output,\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Utilise GPU when avalible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# check if this is redundeant when the above is given\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print('Running on {}'.format(device))\n",
    "\n",
    "# enable parallel processing\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# set loss fuction\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# set optimiser function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every step_size\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T21:31:19.368905Z",
     "start_time": "2019-03-02T21:31:19.342747Z"
    }
   },
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:28:59.330238Z",
     "start_time": "2019-03-03T13:28:59.296310Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    print('Running on {}\\n'.format(device))\n",
    "    \n",
    "    # keep track of historical ROC AUC scores\n",
    "    rocauc_history = []\n",
    "    \n",
    "    # keep track of time spend\n",
    "    since = time.time()\n",
    "    \n",
    "    # init current best model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # best perforamce metrics\n",
    "    best_roc = 0.0\n",
    "    best_acc = 0.0\n",
    "    model_loss = None\n",
    "    \n",
    "    num_updates = 0\n",
    "    \n",
    "    # loop through number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # each epoch contains of a train and valdiation phase \n",
    "        # where validation is done on validation set\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "            # Set NN to training\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "                loader = train_loader\n",
    "                loader_len = len(loader.dataset)\n",
    "\n",
    "            # Set NN to evaluate\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = validation_loader\n",
    "                loader_len = len(loader.dataset)\n",
    "            \n",
    "            # scores of current phase and epoch\n",
    "            running_loss = 0.0\n",
    "            running_roc = 0.0\n",
    "            running_acc = 0.0\n",
    "            \n",
    "            # loop trough minibatches\n",
    "            for batch, (data, target) in enumerate(loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # Forward Propagation\n",
    "                    output = model(data)\n",
    "\n",
    "                    # loss function\n",
    "                    loss = criterion(output.squeeze(), target)\n",
    "\n",
    "                    prediction = (output.data).float()\n",
    "                    y_hat_prob = prediction.cpu().numpy().squeeze()\n",
    "                    \n",
    "                    y_hat_class = (prediction.cpu() > 0.5).float()\n",
    "\n",
    "                    target_y = target.cpu().data.numpy()\n",
    "\n",
    "                    # get predicted labels\n",
    "                    _, preds = torch.max(output, 1)\n",
    "\n",
    "                    # optimise in training\n",
    "                    if phase == 'train':\n",
    "\n",
    "                        # Backwards propagation error\n",
    "                        loss.backward()\n",
    "\n",
    "                        # apply\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                running_roc += roc_auc_score(target_y,\n",
    "                                             y_hat_prob) * data.size(0)\n",
    "                running_acc += balanced_accuracy_score(\n",
    "                    target_y, y_hat_class) * data.size(0)\n",
    "\n",
    "                if (batch + 1) % 50 == 0:\n",
    "                    print('{} Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                        phase, epoch, (batch + 1) * len(data),\n",
    "                        len(loader.dataset), 100. * (batch + 1) / len(loader)))\n",
    "                    \n",
    "            # update epoch scores\n",
    "            epoch_loss = running_loss / loader_len\n",
    "            epoch_roc = running_roc / loader_len\n",
    "            epoch_acc = running_acc / loader_len\n",
    "\n",
    "            print(\n",
    "                '\\n\\t{}:\\tLoss {:.5f},\\tROC AUC {:.5f},\\tBalanced Acc {:.5f}'.\n",
    "                format(phase, epoch_loss, epoch_roc, epoch_acc))\n",
    "\n",
    "            if (phase == 'val'):\n",
    "\n",
    "                # keep track of ROC AUC development\n",
    "                rocauc_history.append(epoch_roc)\n",
    "\n",
    "                if (model_loss == None):\n",
    "                    model_loss = epoch_loss\n",
    "\n",
    "                # deep copy the model\n",
    "                if epoch_roc >= best_roc:\n",
    "                    print('\\t\\tsave updated model')\n",
    "                    best_roc = epoch_roc\n",
    "                    model_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    num_updates += 1\n",
    "\n",
    "    print('\\n')\n",
    "    print('=' * 30)\n",
    "    print('=' * 30)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\n",
    "        'Model Loss:\\t{:4f}\\nROC AUC:\\t{:4f}\\nAccuracy:\\t{:4f}\\nModel updates:\\t{}'\n",
    "        .format(model_loss, best_roc, best_acc, num_updates))\n",
    "\n",
    "    filename = 'model_roc_{:.3f}_balanced_acc_{:.3f}_model_loss_{:.3f}.pt'.format(\n",
    "        best_roc, best_acc, model_loss)\n",
    "\n",
    "    # load best model weights and return this model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, filename, rocauc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:29:25.298527Z",
     "start_time": "2019-03-03T13:29:01.979329Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "best_model, filename, history = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=2)\n",
    "\n",
    "# save model to drive\n",
    "file_poiter = '/content/gdrive/My Drive/Colab Notebooks/' + filename\n",
    "torch.save(best_model, file_poiter)\n",
    "\n",
    "# diagnostic plot\n",
    "plt.plot(history)\n",
    "plt.title('ROC AUC evaluation over time')\n",
    "plt.ylabel('ROC AUC')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on entire train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T22:47:24.496478Z",
     "start_time": "2019-03-03T22:46:48.632881Z"
    }
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('preprocessed.h5', 'r')\n",
    "train_hdf = store.get_storer('train')\n",
    "\n",
    "chunks = np.array_split(np.arange(0,train_hdf.shape[0]), 5)\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    print('/'*100)\n",
    "    print('chunk {}'.format(i))\n",
    "    print('/'*100)\n",
    "    print('')\n",
    "    start = chunk[0]\n",
    "    stop = chunk[-1]\n",
    "    del(chunk)\n",
    "\n",
    "    sub_frame = store.select('train',start=start,stop=stop)\n",
    "    \n",
    "    X_train_chunk = sub_frame.drop(columns=['click', 'payprice']).values\n",
    "    y_train_chunk = sub_frame['click'].values\n",
    "    \n",
    "    del(train_loader)\n",
    "    train_loader = TensorDataset(torch.from_numpy(X_train_chunk).float(), torch.from_numpy(y_train_chunk).float())\n",
    "    train_loader = DataLoader(train_loader, num_workers=4, batch_size=50000, shuffle=True)\n",
    "    \n",
    "    model, filename, history = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=2)\n",
    "    \n",
    "    file_poiter = '/content/gdrive/My Drive/Colab Notebooks/' + filename\n",
    "    torch.save(best_model, file_poiter)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:31:37.220012Z",
     "start_time": "2019-03-03T13:31:37.138993Z"
    }
   },
   "outputs": [],
   "source": [
    "file = 'nmodel_roc_0.851_balanced_acc_0.000_model_loss_0.012.pt'\n",
    "\n",
    "if colab_env:\n",
    "    PATH = '/content/gdrive/My Drive/Colab Notebooks/' + file\n",
    "    model = torch.load(PATH)\n",
    "else:\n",
    "    PATH = '/Users/davidvanrooij/Google Drive/Colab Notebooks/' + file\n",
    "    print(PATH)\n",
    "    model = torch.load(PATH, map_location='cpu')\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:29:47.084479Z",
     "start_time": "2019-03-03T13:29:39.557711Z"
    }
   },
   "outputs": [],
   "source": [
    "batches = np.array_split(validation, 200)\n",
    "\n",
    "pCTR = []\n",
    "for batch in batches:\n",
    "    data = batch.drop(columns=['click', 'payprice']).values\n",
    "    target = batch['click'].values\n",
    "    \n",
    "    data = torch.from_numpy(data).float()\n",
    "    target = torch.from_numpy(target).float()\n",
    "    \n",
    "    output = model(data)\n",
    "    \n",
    "    prediction = (output.data).float()\n",
    "    y_hat_prob = prediction.cpu().numpy().squeeze()\n",
    "    \n",
    "    pCTR.extend(y_hat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T13:29:49.399924Z",
     "start_time": "2019-03-03T13:29:49.341770Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payprice = validation['payprice']\n",
    "clicks = validation['click']\n",
    "\n",
    "no_click, click = np.bincount(clicks)\n",
    "avgCTR = click / (no_click + click)\n",
    "\n",
    "bid_strategy = lambda lamda, const, pCTR: np.sqrt(np.multiply((const / lamda), pCTR) + const**2) - const\n",
    "\n",
    "lamda_range = np.linspace(1e-7, 1e-4, num=20)\n",
    "const_range = np.arange(10, 100, 10)\n",
    "\n",
    "parameter_grid = [(x,i) for x in const_range for i in lamda_range]\n",
    "# bid_range_wide = np.arange(1, 5, .5)\n",
    "# bid_range = np.concatenate((bid_range, bid_range_wide), axis=0)\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "# find optimal base_bid\n",
    "for const, lamda in tqdm(parameter_grid):\n",
    "    budget_remaining = 6250*1000\n",
    "    \n",
    "    index = (const, lamda)\n",
    "    \n",
    "    statistics[index] = {\n",
    "        'impressions':0,\n",
    "        'spend': 0,\n",
    "        'clicks': 0,\n",
    "        'too_expensive': 0\n",
    "    }\n",
    "    \n",
    "    # list of bids for all ad requests\n",
    "    bids = bid_strategy(lamda, const, pCTR)\n",
    "    \n",
    "    # loop through all bids for every ad request\n",
    "    for i in range(len(bids)):    \n",
    "        second_highest_bid = payprice[i]\n",
    "        \n",
    "        won = bids[i] >= second_highest_bid and second_highest_bid <= budget_remaining\n",
    "        \n",
    "        if(second_highest_bid > budget_remaining):\n",
    "            statistics[index]['too_expensive'] += 1\n",
    "        \n",
    "        \n",
    "        if(won):\n",
    "            statistics[index]['impressions'] += 1\n",
    "            statistics[index]['spend'] += second_highest_bid\n",
    "            statistics[index]['clicks'] += clicks[i]\n",
    "            \n",
    "            # subtract current bid from budget \n",
    "            budget_remaining -= second_highest_bid\n",
    "            \n",
    "    \n",
    "statistics = pd.DataFrame(statistics).T\n",
    "statistics['CTR'] = statistics['clicks'] / statistics['impressions']\n",
    "statistics['aCPM'] = statistics['spend'] / statistics['impressions'] \n",
    "statistics['aCPC'] = (statistics['spend']/1000) / statistics['clicks']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:04:19.161779Z",
     "start_time": "2019-03-01T14:03:15.066Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics.sort_values('clicks', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NN to predict pCTR on test and apply non-linear bidding strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = np.array_split(test, 200)\n",
    "\n",
    "pCTR_test = []\n",
    "for batch in batches:\n",
    "    data = batch.drop(columns=['click', 'payprice']).values\n",
    "    target = batch['click'].values\n",
    "    \n",
    "    data = torch.from_numpy(data).float()\n",
    "    target = torch.from_numpy(target).float()\n",
    "    \n",
    "    output = model(data)\n",
    "    \n",
    "    prediction = (output.data).float()\n",
    "    y_hat_prob = prediction.cpu().numpy().squeeze()\n",
    "    \n",
    "    pCTR_test.extend(y_hat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns=['click', 'payprice'])\n",
    "\n",
    "bids = bid_strategy(0.000016, 30, pCTR_test)\n",
    "\n",
    "# get bid id's\n",
    "test_raw = pd.read_csv(\n",
    "    '/content/gdrive/My Drive/Colab Notebooks/we_data/test.csv')\n",
    "\n",
    "# export to file\n",
    "df_bids = pd.DataFrame(\n",
    "    np.round(bids, 1), index=test_raw['bidid'].values, columns=['bidprice'])\n",
    "df_bids.index.name = 'bidid'\n",
    "df_bids = df_bids.reset_index()\n",
    "\n",
    "file = '/content/gdrive/My Drive/Colab Notebooks/bid_attemnt_{}.csv'.format(\n",
    "    time.strftime('%Y-%m-%d_%H:%M:%S'))\n",
    "df_bids.to_csv(file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464px",
    "left": "1345px",
    "right": "14px",
    "top": "118px",
    "width": "321px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
