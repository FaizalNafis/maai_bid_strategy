{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:19.051102Z",
     "start_time": "2019-02-26T17:30:08.460287Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T12:44:55.595650Z",
     "start_time": "2019-02-24T12:44:55.592418Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:19.164347Z",
     "start_time": "2019-02-26T17:30:19.154175Z"
    }
   },
   "outputs": [],
   "source": [
    "def memory(df):\n",
    "    print(\"Memory usage of the dataframe is {:.2f} MB\".format(\n",
    "        df.memory_usage().sum() / 1024**2))\n",
    "    \n",
    "    \n",
    "def entropy(df, base = 2):\n",
    "    \"\"\" Calculate the entropy for every column in a df\"\"\"\n",
    "    \n",
    "    entropy = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        prob = df[column].value_counts(normalize=True, sort=False)\n",
    "        \n",
    "        entropy[column] = -(prob * np.log(prob)/np.log(base)).sum()\n",
    "        \n",
    "    return pd.Series(entropy).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:48.716860Z",
     "start_time": "2019-02-26T17:30:19.268550Z"
    }
   },
   "outputs": [],
   "source": [
    "google_drive = False\n",
    "convert = False\n",
    "\n",
    "if(google_drive):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/we_data/train.csv')\n",
    "    \n",
    "elif(convert):\n",
    "    train = pd.read_csv('../we_data/train.csv')\n",
    "    train.to_hdf('train.h5', 'train')\n",
    "    \n",
    "else:\n",
    "    train = pd.read_hdf('../train.h5', 'train')\n",
    "    validation = pd.read_csv('../we_data/validation.csv')\n",
    "    test = pd.read_csv('../we_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.127863Z",
     "start_time": "2019-02-26T17:30:49.772148Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-629914d6d1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.151412Z",
     "start_time": "2019-02-26T17:30:19.226Z"
    }
   },
   "outputs": [],
   "source": [
    "train['adexchange'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.184584Z",
     "start_time": "2019-02-26T17:30:19.882Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "other fillna strategies for adexchange should be considered at some point!\n",
    "\n",
    "the only columns that contain a lot of missing values that are used in the final \n",
    "analysis are adexchange and usertag. Different strategies have been considered but \n",
    "it was deemed to be the most informative to assign a 'unknown' class which is easely\n",
    "achieved trhough filling 0's since they do not occur in the dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.206838Z",
     "start_time": "2019-02-26T17:30:20.659Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.223847Z",
     "start_time": "2019-02-26T17:30:21.508Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.259259Z",
     "start_time": "2019-02-26T17:30:23.203Z"
    }
   },
   "outputs": [],
   "source": [
    "no_click,click = train['click'].value_counts().values\n",
    "print('Baseline average CTR {:.5f}'.format(click/no_click))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T17:30:55.443Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_featuresize_one_hot_encoding(df):\n",
    "    \"\"\"Calcualte the number of featuers nessecary for one hot encoding\"\"\"\n",
    "\n",
    "    total_features = 0\n",
    "    for column in df.columns:\n",
    "        total_features += len(df[column].unique())\n",
    "\n",
    "    print('Rougly {:,} features in the feature space'.format(total_features))\n",
    "\n",
    "    return total_features\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\" Enrich dataframe with additional features\n",
    "    \n",
    "        Note that all fields that are joined are slightly redundent when\n",
    "        implementing more sophisticated models like NN that could pick up\n",
    "        on these feature combinations, however, it can improve the perforamce\n",
    "        of simpler models such as logisitc regression\"\"\"\n",
    "\n",
    "    # split user agent into os and browser\n",
    "    df['os'], df['browser'] = df['useragent'].str.split('_').str\n",
    "\n",
    "    # apple users\n",
    "    df['apple'] = df['useragent'].str.match(r'(ios)|(mac)').astype(np.uint8)\n",
    "\n",
    "    # deterime mobile devivce or not\n",
    "    df['mobieldevice'] = df['useragent'].str.match(r'(ios)|(android)').astype(\n",
    "        np.uint8)\n",
    "\n",
    "    # hour per day\n",
    "    df['weekdayhour'] = df['weekday'].astype(str) + '_' + df['hour'].astype(\n",
    "        str)\n",
    "\n",
    "    # mobile per day of the week\n",
    "    df['mobileweekday'] = df['mobieldevice'].astype(\n",
    "        str) + '_' + df['weekday'].astype(str)\n",
    "\n",
    "    # mobile per day of the week\n",
    "    df['mobilehour'] = df['mobieldevice'].astype(\n",
    "        str) + '_' + df['hour'].astype(str)\n",
    "\n",
    "    # brouwser per day of the week\n",
    "    df['browserweekday'] = df['browser'] + '_' + df['weekday'].astype(str)\n",
    "\n",
    "    # brouwser per day of the week\n",
    "    df['browserhour'] = df['browser'] + '_' + df['hour'].astype(str)\n",
    "\n",
    "    # os per day of the week\n",
    "    df['osweekday'] = df['os'] + '_' + df['weekday'].astype(str)\n",
    "\n",
    "    # os per hour\n",
    "    df['oshour'] = df['os'] + '_' + df['hour'].astype(str)\n",
    "\n",
    "    # os per day per hour\n",
    "    df['osdayhour'] = df['os'] + '_' + df['weekday'].astype(\n",
    "        str) + '_' + df['hour'].astype(str)\n",
    "\n",
    "    # bin hours into time of day\n",
    "    df['timeofday'] = pd.cut(\n",
    "        df['hour'].astype(int),\n",
    "        4,\n",
    "        labels=[\"night\", \"morning\", \"afternoon\", \"evening\"])\n",
    "\n",
    "    # bin ad surface size categories\n",
    "    min_ad = min(df['slotwidth'] * df['slotheight']) - 1\n",
    "    max_ad = max(df['slotwidth'] * df['slotheight'])\n",
    "\n",
    "    ad_bins = pd.IntervalIndex.from_breaks(\n",
    "        [min_ad, 65520, 75000, 90000, max_ad])\n",
    "\n",
    "    replace, with_ = [\n",
    "        pd.Interval(min_ad, 65520),\n",
    "        pd.Interval(65520, 75000),\n",
    "        pd.Interval(75000, 90000),\n",
    "        pd.Interval(90000, max_ad)\n",
    "    ], ['small', 'medium', 'large', 'x-large']\n",
    "\n",
    "    df['adsize'] = pd.cut(\n",
    "        df['slotwidth'] * df['slotheight'], bins=ad_bins).replace(\n",
    "            replace, with_)\n",
    "\n",
    "    # bin slot price into categories\n",
    "    price_bins = pd.IntervalIndex.from_breaks(\n",
    "        [min(df['slotprice']), 10, 50, 100,\n",
    "         max(df['slotprice'])],\n",
    "        closed='left')\n",
    "    replace, with_ = [\n",
    "        pd.Interval(min(df['slotprice']), 10, closed='left'),\n",
    "        pd.Interval(10, 50, closed='left'),\n",
    "        pd.Interval(50, 100, closed='left'),\n",
    "        pd.Interval(100, max(df['slotprice']), closed='left')\n",
    "    ], ['1', '2', '3', '4']\n",
    "\n",
    "    df['slotprice'] = pd.cut(\n",
    "        df['slotprice'], bins=price_bins).replace(replace, with_)\n",
    "\n",
    "    # ad size category and visability\n",
    "    df['advisabilitysize'] = df['slotvisibility'].astype(\n",
    "        str) + '_' + df['adsize'].astype(str)\n",
    "\n",
    "    return df.drop(columns=['slotwidth', 'slotheight'])\n",
    "\n",
    "\n",
    "def pre_process_one_hot_encoding(df):\n",
    "    \"\"\" Preprocess the dataframe for one hot encoding\n",
    "    \n",
    "        - Split the filed user tags and binary encode\n",
    "        - Convert numerical categories into strings \n",
    "        \"\"\"\n",
    "\n",
    "    # convert numerical categories into strings as a quick hack\n",
    "    # for one hot encoding to work properly on numerical categories\n",
    "    df['weekday'] = df['weekday'].astype(str)\n",
    "    df['hour'] = df['hour'].astype(int)\n",
    "    df['region'] = df['region'].astype(str)\n",
    "    df['city'] = df['city'].astype(str)\n",
    "    df['adexchange'] = df['adexchange'].astype(str)\n",
    "\n",
    "    # already processed and not found in columns\n",
    "    if ('usertag' not in df.columns):\n",
    "        return df\n",
    "\n",
    "    df['usertag'] = df['usertag'].astype(str)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    df = df.join(\n",
    "        pd.DataFrame(\n",
    "            mlb.fit_transform(df['usertag'].str.split(',')),\n",
    "            columns='usertag_' + mlb.classes_,\n",
    "            index=df.index))\n",
    "\n",
    "    # drop the usertag column\n",
    "    df = df.drop(columns='usertag')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_missing_colums(df, columns, sort_columns=True):\n",
    "    \"\"\" Due to the feature engineering there is a chance a discrapency occurs\n",
    "        between \n",
    "        \"\"\"\n",
    "\n",
    "    missing = [x for x in columns if x not in df.columns]\n",
    "\n",
    "    for x in missing:\n",
    "        df[x] = 0\n",
    "\n",
    "    if (sort_columns):\n",
    "        return df[sorted(df.columns)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_colums(df):\n",
    "\n",
    "    # Remove uniuqe and meaningless featuers that are not know a pirori\n",
    "    columns = ['bidprice', 'urlid', 'bidid']\n",
    "\n",
    "    # remove some very sparse fields to reduce featuers (highest entropy)\n",
    "    columns.extend(['userid', 'url', 'domain', 'slotid', 'IP'])\n",
    "\n",
    "    # only remove columns that are in the df\n",
    "    columns = [column for column in columns if column in df.columns]\n",
    "\n",
    "    return df.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.320456Z",
     "start_time": "2019-02-26T17:30:26.723Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcluate_num_impressions(df, grouping = False):\n",
    "    imp = {}\n",
    "    \n",
    "    if(grouping):\n",
    "        imp = df.groupby(grouping).size().to_dict()\n",
    "            \n",
    "    else:\n",
    "        imp = len(df)\n",
    "    \n",
    "    print(imp)\n",
    "    return imp\n",
    "    \n",
    "\n",
    "\n",
    "def calcluate_num_clicks(df, grouping = False):\n",
    "    clicks = {}\n",
    "    \n",
    "    if(grouping):\n",
    "        for index, group in df.groupby(grouping):\n",
    "            clicks[index] = np.sum(group['click'])\n",
    "            \n",
    "    else:\n",
    "        clicks = np.sum(df['click'])\n",
    "    \n",
    "    print(clicks)\n",
    "    return clicks\n",
    "    \n",
    "\n",
    "def calcluate_ctr(df, grouping = False):\n",
    "    ctr = {}\n",
    "    \n",
    "    if(grouping):\n",
    "        for index, group in df.groupby(grouping):\n",
    "            ctr[index] = np.average(group['click']) * 100\n",
    "            \n",
    "    else:\n",
    "        ctr = np.average(df['click']) * 100\n",
    "    \n",
    "    print(ctr)\n",
    "    return ctr\n",
    "    \n",
    "    \n",
    "def average_cost_per_mille(df, grouping = False):\n",
    "    cpm = {}\n",
    "    \n",
    "    if(grouping):\n",
    "        cpm = df.groupby('weekday').agg({'payprice': np.mean}).to_dict()['payprice']\n",
    "        \n",
    "    else:\n",
    "        cpm = np.mean(df['payprice'])\n",
    "        \n",
    "    print(cpm)\n",
    "    return cpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:30:50.363384Z",
     "start_time": "2019-02-26T17:30:28.240Z"
    }
   },
   "outputs": [],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T17:30:51.870Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drop_colums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drop_colums' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.59 s, sys: 1.12 s, total: 4.71 s\n",
      "Wall time: 5.94 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_engineering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_engineering' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pre_process_one_hot_encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_process_one_hot_encoding' is not defined"
     ]
    }
   ],
   "source": [
    "%time train = drop_colums(train)\n",
    "%time train = train.fillna(0)\n",
    "%time train = feature_engineering(train)\n",
    "%time train = pre_process_one_hot_encoding(train)\n",
    "%time train = pd.get_dummies(train)\n",
    "\n",
    "memory(train)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T12:30:23.945Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv('../we_data/train_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T14:00:01.232862Z",
     "start_time": "2019-02-24T13:59:54.849285Z"
    }
   },
   "outputs": [],
   "source": [
    "%time test = drop_colums(test)\n",
    "%time test = test.fillna(0)\n",
    "%time test = pre_process_one_hot_encoding(test)\n",
    "%time test = feature_engineering(test)\n",
    "%time test = pd.get_dummies(test)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T12:23:25.029Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time validation = drop_colums(validation)\n",
    "%time validation = validation.fillna(0)\n",
    "%time validation = feature_engineering(validation)\n",
    "%time validation = pre_process_one_hot_encoding(validation)\n",
    "%time validation = pd.get_dummies(validation)\n",
    "# train.to_csv('/we_data/validation_preprocessed.csv')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T14:00:12.543061Z",
     "start_time": "2019-02-24T14:00:02.709338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['osdayhourios_2_19', 'osdayhourios_1_1', 'osdayhourios_0_2', 'osdayhourios_0_11']\n"
     ]
    }
   ],
   "source": [
    "# find features/columns that occur in test set but not in trainset\n",
    "unique_test = []\n",
    "for column in test.columns:\n",
    "    for x in test[column].unique():\n",
    "        unique_test.append(str(column) + str(x))\n",
    "        \n",
    "unique_train = []\n",
    "for column in train.columns:\n",
    "    for x in train[column].unique():\n",
    "        unique_train.append(str(column) + str(x))\n",
    "\n",
    "# values in test set but not in trainset\n",
    "print([x for x in unique_test if x not in unique_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T14:44:16.406394Z",
     "start_time": "2019-02-24T14:41:29.615Z"
    }
   },
   "outputs": [],
   "source": [
    "# find features/columns that occur in validation set but not in trainset\n",
    "unique_validation = []\n",
    "for column in validation.columns:\n",
    "    for x in validation[column].unique():\n",
    "        unique_validation.append(str(column) + str(x))\n",
    "        \n",
    "unique_train = []\n",
    "for column in train.columns:\n",
    "    for x in train[column].unique():\n",
    "        unique_train.append(str(column) + str(x))\n",
    "\n",
    "# values in validation set but not in trainset\n",
    "print([x for x  in unique_validation if x not in unique_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-25T17:21:18.261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 45.3 s, total: 1min 7s\n",
      "Wall time: 1min 37s\n",
      "CPU times: user 4.61 s, sys: 6.88 s, total: 11.5 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "# the featuere engineering can construct columns that do not occur in other sets \n",
    "# this adds the columns of the joined colomuns\n",
    "joined_colums = [item for slist in [validation.columns, train.columns, test.columns] for item in slist]\n",
    "\n",
    "%time train = add_missing_colums(train,joined_colums)\n",
    "%time validation = add_missing_colums(validation,joined_colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-25T17:21:20.031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in validation.columns if x not in train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION 1: Apply bloom filter on all filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-20T16:45:05.190Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_dict = df_copy.drop(columns='click').T.to_dict().values()\n",
    "# h = FeatureHasher(n_features=20000)\n",
    "# maxtix = h.transform(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T16:44:59.857009Z",
     "start_time": "2019-02-20T16:44:59.673023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maxtix.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION 2: Get feature matrix of select number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Balance dataset\n",
    "\n",
    "There is a significant unbalance between the click and the non-click class. In an attempt to increase numerical stability of the training models the majority class will be under sampled (observations will be removed) and the minority class will be oversampled generating new samples. Considering a sample x_i, a new sample x_{new} will be generated considering its k neareast-neighbors. \n",
    "\n",
    "SMOTEENN Combines over and under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-25T17:21:44.939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "0    2429188\n",
      "1       1793\n",
      "Name: click, dtype: int64\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4199\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   4200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4201\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random subsample class0:\n",
      "0    2429188\n",
      "1       1793\n",
      "Name: click, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = train\n",
    "\n",
    "print('Original:')\n",
    "print(training_data['click'].value_counts())\n",
    "print('')\n",
    "# random sample from the non click class, pre subsameling will improve speed \n",
    "# since SMOTETomek does not take an output number samples as parameter\n",
    "class0 = training_data.query('click == 0')\n",
    "%time class0 = class0.sample(n=4e4)\n",
    "class1 = training_data.query('click == 1')\n",
    "\n",
    "# combine dataframes\n",
    "training_data = pd.concat([class0,class1])\n",
    "\n",
    "print('Random subsample class0:')\n",
    "print(training_data['click'].value_counts())\n",
    "print('')\n",
    "\n",
    "smote_omek = SMOTETomek(random_state=0, ratio=1/8)\n",
    "\n",
    "y = training_data['click'].values\n",
    "X = training_data.drop(columns=['click', 'payprice'])\n",
    "\n",
    "%time X_resampled, y_resampled = smote_omek.fit_resample(X, y)\n",
    "\n",
    "print('Final balance:')\n",
    "sample, count = np.unique(y_resampled, return_counts=True)\n",
    "print(pd.Series(dict(np.array((sample,count)).T)))\n",
    "\n",
    "print('\\nBaseline accuracy: {:.3%}'.format(count[0]/len(y_resampled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation accuracy\n",
    "Takes a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T16:27:46.806766Z",
     "start_time": "2019-02-25T16:19:13.808329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 50s, sys: 19.1 s, total: 15min 9s\n",
      "Wall time: 8min 32s\n",
      "Average accuracy: 99.383% (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', max_iter=1e4) # use solver=sag when training on the entire dataset\n",
    "\n",
    "%time scores = cross_val_score(lr, X_resampled, y_resampled, cv=10) #error_score='f1',\n",
    "print(\"Average accuracy: {:.3%} (+/- {:.2f})\".format(np.mean(scores), np.std(scores) * 2))\n",
    "\n",
    "# lr.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-25T17:21:49.592Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', max_iter=1e4) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)\n",
    "%time lr.fit(X_train, y_train)\n",
    "\n",
    "y_hat = lr.predict(X_test)\n",
    "print(\"Balanced accuracy score: {:.3%}\".format(balanced_accuracy_score(y_test, y_hat)  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-25T17:21:51.532Z"
    }
   },
   "outputs": [],
   "source": [
    "y_validate = validation['click'].values\n",
    "X_validate = validation.drop(columns=['click', 'payprice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T17:03:32.636Z"
    }
   },
   "outputs": [],
   "source": [
    "print(kek)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "435px",
    "left": "1135px",
    "right": "73px",
    "top": "126px",
    "width": "477px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
