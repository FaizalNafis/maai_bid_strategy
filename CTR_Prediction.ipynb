{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "# from pylab import rcParams\n",
    "#Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "#Evaluation & Plotting\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from fastFM import als\n",
    "import scipy.sparse as sp\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "from sknn.mlp import Classifier, Layer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(df, pred, model= None):\n",
    "    \"\"\"\n",
    "    Function to plot ROC\n",
    "    \"\"\"\n",
    "    \n",
    "    #Compute false positive rate, true positive rate and threshholds using the roc_curve method\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(df, pred)\n",
    "    auc = roc_auc_score(df,pred)\n",
    "    \n",
    "    if model != None:\n",
    "        label_title = '%s (AUC =%f)' % (model, auc)\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    ax, fig = plt.subplots(figsize= (12,8))\n",
    "    plt.tick_params(labelsize=5)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label=label_title)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate', fontsize=7)\n",
    "    plt.ylabel('True Positive Rate', fontsize=7)\n",
    "    plt.title(plot_title, fontsize=9)\n",
    "    plt.legend(loc=\"lower right\", fontsize=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_df, val_df, parameters = {'max_features': [], 'n_estimators':[], 'min_sample_leaf' :[]}\n",
    "                  \n",
    " \"\"\"\n",
    " 1. max features: the maximum number of features RF is allowed to try in individual tree, sqrt = the model will take a sqt\n",
    " of the total number of features in each instance\n",
    " - Increasing the number of max features increases performance for each tree, but reduces diversity in the random forest\n",
    " \n",
    " 2. n_estimators: number of trees \n",
    " \n",
    " 3. min_sample_leaf: leaf is the end node of a decision mode, hence the smaller the leaf, the model is more prone to \n",
    " capturing noise in train_df \n",
    " \n",
    "       \n",
    "    return model, prediction[:,1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
